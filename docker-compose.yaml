

volumes:
  redis-data:
  weaviate-data:

services:
# ────────────── DATA LAYER ──────────────
  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "60", "1", "--loglevel", "warning"]
    volumes:
      - ./data/redis:/data
    restart: unless-stopped

  weaviate:
    image: semitechnologies/weaviate:1.24.8
    ports: ["8080:8080"]
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"

      DEFAULT_VECTORIZER_MODULE: text2vec-openai
      ENABLE_MODULES: text2vec-openai,text2vec-huggingface
      OPENAI_APIKEY: ${OPENAI_API_KEY:-}
    volumes:
      - ./data/weaviate:/var/lib/weaviate
    restart: unless-stopped
    depends_on: [redis]

# ────────────── MCP FAÇADES ──────────────
  mcp-openshift:
    build: ./mcp-openshift
    environment:
      MCP_USERNAME: ${MCP_USERNAME:-admin}
      MCP_PASSWORD: ${MCP_PASSWORD:-secret}
      KUBECONFIG: /app/.kube/config
    ports: ["9000:9000"]
    restart: unless-stopped

  mcp-fmp:
    build: ./mcp-fmp
    environment:
      MCP_USERNAME: ${MCP_USERNAME:-admin}
      MCP_PASSWORD: ${MCP_PASSWORD:-secret}
      FMP_API_KEY:  ${FMP_API_KEY}
    ports: ["9100:9000"]
    restart: unless-stopped

# ────────────── AGENT (FastAPI + RAG) ──────────────
  agent-server:
    build: .
    ports: ["8000:8000"]
    environment:
      LLM_BACKEND:  ${LLM_BACKEND:-openai}
      LLM_BASE_URL: ${LLM_BASE_URL:-}
      LLM_MODEL:    ${LLM_MODEL}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}

      REDIS_HOST:   redis
      WEAVIATE_URL: http://weaviate:8080
      MCP_FMP_URL:  http://mcp-fmp:9000
      MCP_K8S_URL:  http://mcp-openshift:9000
      MCP_USERNAME: ${MCP_USERNAME:-admin}
      MCP_PASSWORD: ${MCP_PASSWORD:-secret}
      ADMIN_TOKEN:  ${ADMIN_TOKEN:-supersecrettoken}

    volumes:
      - ./:/app
    command: >
      uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped
    depends_on: [redis, weaviate, mcp-openshift, mcp-fmp]

# ────────────── SIMPLE STREAMLIT UI ──────────────
  ui:
    build: ./ui
    ports: ["8501:8501"]
    environment:
      BACKEND_URL: http://agent-server:8000
      AUTH_TOKEN:  ${ADMIN_TOKEN:-supersecrettoken}
      STREAMLIT_SERVER_RUNONSAVE: true
    volumes:
      - ./ui:/app
    depends_on: [agent-server]

# ────────────── OPTIONAL: LOCAL vLLM SERVER ──────────────
#  vllm:
#    image: vllm/vllm-openai:latest
#    command: python -m vllm.entrypoints.openai.api_server \
#             --model /models/llama-3-8b-instruct.Q4_K_M.gguf \
#             --served-model-name llama-3-8b-instruct
#    volumes:
#      - /path/to/gguf:/models
#    ports: ["8000:8000"]
