# ────────────── Named volumes (persistent) ──────────────
volumes:
  redis-data:
  weaviate-data:

services:
# ────────────── DATA LAYER ──────────────
  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "60", "1", "--loglevel", "warning"]
    volumes:
      - redis-data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    ports: ["6379:6379"]        # ← host-level access (optional)
    expose: ["6379"]            # internal network (redundant if ports above)

  weaviate:
    image: semitechnologies/weaviate:1.24.8
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      DEFAULT_VECTORIZER_MODULE: text2vec-openai
      ENABLE_MODULES: text2vec-openai,text2vec-huggingface
      OPENAI_APIKEY: ${OPENAI_API_KEY:-}
    volumes:
      - weaviate-data:/var/lib/weaviate
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    expose: ["8080"]             # internal use
    ports: ["8080:8080"]         # reach Weaviate from host if needed

# ────────────── MCP SERVERS ──────────────
  mcp-openshift:
    build: ./mcp-openshift
    environment:
      MCP_USERNAME: ${MCP_USERNAME:-admin}
      MCP_PASSWORD: ${MCP_PASSWORD:-secret}
      KUBECONFIG: /app/.kube/config
      PORT: 9000
    expose: ["9000"]             # only agent needs this
    ports: ["9101:9000"]         # expose to host (optional)
    restart: unless-stopped

  mcp-fmp:
    build: ./mcp-fmp
    environment:
      PORT: 5000
      FMP_API_KEY: ${FMP_API_KEY}
    expose: ["5000"]             # agent uses this internally
    ports:
      - "${HOST_PORT:-9100}:5000"  # host access → localhost:9100
    restart: unless-stopped

# ────────────── AGENT (FastAPI + RAG) ──────────────
  agent-server:
    build: .
    environment:
      HUGGINGFACE_HUB_TOKEN: ${HUGGINGFACE_HUB_TOKEN}
      LLM_BACKEND:  ${LLM_BACKEND:-openai}
      LLM_BASE_URL: ${LLM_BASE_URL:-}
      LLM_MODEL:    ${LLM_MODEL}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}

      REDIS_HOST:   redis
      WEAVIATE_URL: http://weaviate:8080
      FMP_FASTAPI_URL:  http://mcp-fmp:5000     # ← match fmp_tools.py
      MCP_K8S_URL:  http://mcp-openshift:9000
      MCP_USERNAME: ${MCP_USERNAME:-admin}
      MCP_PASSWORD: ${MCP_PASSWORD:-secret}
      ADMIN_TOKEN:  ${ADMIN_TOKEN:-supersecrettoken}

    volumes:
      - ./:/app
    command: >
      uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    restart: unless-stopped
    depends_on:
      weaviate:
        condition: service_started
      mcp-openshift:
        condition: service_started
      mcp-fmp:
        condition: service_started
    expose: ["8000"]             # internal (ui)
    ports: ["8000:8000"]         # host access for API

# ────────────── SIMPLE STREAMLIT UI ──────────────
  ui:
    build: ./ui
    environment:
      BACKEND_URL: http://agent-server:8000
      AUTH_TOKEN:  ${ADMIN_TOKEN:-supersecrettoken}
      STREAMLIT_SERVER_RUNONSAVE: "true"
    volumes:
      - ./ui:/app
    depends_on:
      agent-server:
        condition: service_started
    ports: ["8501:8501"]         # expose UI to host
